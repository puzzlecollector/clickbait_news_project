{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "import torch \n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F \n",
    "from transformers import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import log_loss, auc\n",
    "import random \n",
    "import time \n",
    "import datetime \n",
    "from tqdm import tqdm\n",
    "import gc  \n",
    "import seaborn as sns   \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using the simple chunking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./storage/minds_fake_news/mindslab_test.csv') \n",
    "y_test = test['Label'].values    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s): \n",
    "    FILTERS = \"([~.,!?\\\"':;(])\"\n",
    "    CHANGE_FILTER = re.compile(FILTERS)\n",
    "    return re.sub(CHANGE_FILTER, \" \", s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(s, overlap = 20, chunk_size = 50): \n",
    "    total = [] \n",
    "    partial = [] \n",
    "    if len(s.split()) // (chunk_size - overlap) > 0:  \n",
    "        n = len(s.split()) // (chunk_size - overlap) \n",
    "    else: \n",
    "        n = 1 \n",
    "    for w in range(n): \n",
    "        if w == 0: \n",
    "            partial = s.split()[:chunk_size] \n",
    "            total.append(\" \".join(partial)) \n",
    "        else:  \n",
    "            partial = s.split()[w*(chunk_size - overlap):w*(chunk_size - overlap) + chunk_size]\n",
    "            total.append(\" \".join(partial)) \n",
    "    return total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = test['content'].values\n",
    "titles = test['title'].values \n",
    "labels = test['Label'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def electra_tokenizer_simple(sent1, sent2, MAX_LEN):  \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent1, \n",
    "        text_pair = sent2,  \n",
    "        add_special_tokens = True, # add [CLS] and [SEP]\n",
    "        pad_to_max_length = False, \n",
    "        return_attention_mask = True # constructing attention_masks \n",
    "    )  \n",
    "    \n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] # differentiate padding from non padding \n",
    "    token_type_id = encoded_dict['token_type_ids'] # differentiate two sentences    \n",
    "    \n",
    "    if len(input_id) > 512: \n",
    "        input_id = input_id[:129] + input_id[-383:] \n",
    "        attention_mask = attention_mask[:129] + attention_mask[-383:]  \n",
    "        token_type_id = token_type_id[:129] + token_type_id[-383:]   \n",
    "    elif len(input_id) < 512: \n",
    "        input_id = input_id + [0]*(512 - len(input_id)) \n",
    "        attention_mask = attention_mask + [0]*(512 - len(attention_mask))\n",
    "        token_type_id = token_type_id + [0]*(512 - len(token_type_id))  \n",
    "        \n",
    "    return np.asarray(input_id), np.asarray(attention_mask), np.asarray(token_type_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")    \n",
    "checkpoint = torch.load('./storage/electra_chunked_2') \n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction for datapoint 1 ...\n",
      "Making prediction for datapoint 2 ...\n",
      "Making prediction for datapoint 3 ...\n",
      "Making prediction for datapoint 4 ...\n",
      "Making prediction for datapoint 5 ...\n",
      "Making prediction for datapoint 6 ...\n",
      "Making prediction for datapoint 7 ...\n",
      "Making prediction for datapoint 8 ...\n",
      "Making prediction for datapoint 9 ...\n",
      "Making prediction for datapoint 10 ...\n",
      "Making prediction for datapoint 11 ...\n",
      "Making prediction for datapoint 12 ...\n",
      "Making prediction for datapoint 13 ...\n",
      "Making prediction for datapoint 14 ...\n",
      "Making prediction for datapoint 15 ...\n",
      "Making prediction for datapoint 16 ...\n",
      "Making prediction for datapoint 17 ...\n",
      "Making prediction for datapoint 18 ...\n",
      "Making prediction for datapoint 19 ...\n",
      "Making prediction for datapoint 20 ...\n",
      "Making prediction for datapoint 21 ...\n",
      "Making prediction for datapoint 22 ...\n",
      "Making prediction for datapoint 23 ...\n",
      "Making prediction for datapoint 24 ...\n",
      "Making prediction for datapoint 25 ...\n",
      "Making prediction for datapoint 26 ...\n",
      "Making prediction for datapoint 27 ...\n",
      "Making prediction for datapoint 28 ...\n",
      "Making prediction for datapoint 29 ...\n",
      "Making prediction for datapoint 30 ...\n",
      "Making prediction for datapoint 31 ...\n",
      "Making prediction for datapoint 32 ...\n",
      "Making prediction for datapoint 33 ...\n",
      "Making prediction for datapoint 34 ...\n",
      "Making prediction for datapoint 35 ...\n",
      "Making prediction for datapoint 36 ...\n",
      "Making prediction for datapoint 37 ...\n",
      "Making prediction for datapoint 38 ...\n",
      "Making prediction for datapoint 39 ...\n",
      "Making prediction for datapoint 40 ...\n",
      "Making prediction for datapoint 41 ...\n",
      "Making prediction for datapoint 42 ...\n",
      "Making prediction for datapoint 43 ...\n",
      "Making prediction for datapoint 44 ...\n",
      "Making prediction for datapoint 45 ...\n",
      "Making prediction for datapoint 46 ...\n",
      "Making prediction for datapoint 47 ...\n",
      "Making prediction for datapoint 48 ...\n",
      "Making prediction for datapoint 49 ...\n",
      "Making prediction for datapoint 50 ...\n",
      "Making prediction for datapoint 51 ...\n",
      "Making prediction for datapoint 52 ...\n",
      "Making prediction for datapoint 53 ...\n",
      "Making prediction for datapoint 54 ...\n",
      "Making prediction for datapoint 55 ...\n",
      "Making prediction for datapoint 56 ...\n",
      "Making prediction for datapoint 57 ...\n",
      "Making prediction for datapoint 58 ...\n",
      "Making prediction for datapoint 59 ...\n",
      "Making prediction for datapoint 60 ...\n",
      "Making prediction for datapoint 61 ...\n",
      "Making prediction for datapoint 62 ...\n",
      "Making prediction for datapoint 63 ...\n",
      "Making prediction for datapoint 64 ...\n",
      "Making prediction for datapoint 65 ...\n",
      "Making prediction for datapoint 66 ...\n",
      "Making prediction for datapoint 67 ...\n",
      "Making prediction for datapoint 68 ...\n",
      "Making prediction for datapoint 69 ...\n",
      "Making prediction for datapoint 70 ...\n",
      "Making prediction for datapoint 71 ...\n",
      "Making prediction for datapoint 72 ...\n",
      "Making prediction for datapoint 73 ...\n",
      "Making prediction for datapoint 74 ...\n",
      "Making prediction for datapoint 75 ...\n",
      "Making prediction for datapoint 76 ...\n",
      "Making prediction for datapoint 77 ...\n",
      "Making prediction for datapoint 78 ...\n",
      "Making prediction for datapoint 79 ...\n",
      "Making prediction for datapoint 80 ...\n",
      "Making prediction for datapoint 81 ...\n",
      "Making prediction for datapoint 82 ...\n",
      "Making prediction for datapoint 83 ...\n",
      "Making prediction for datapoint 84 ...\n",
      "Making prediction for datapoint 85 ...\n",
      "Making prediction for datapoint 86 ...\n",
      "Making prediction for datapoint 87 ...\n",
      "Making prediction for datapoint 88 ...\n",
      "Making prediction for datapoint 89 ...\n",
      "Making prediction for datapoint 90 ...\n",
      "Making prediction for datapoint 91 ...\n",
      "Making prediction for datapoint 92 ...\n",
      "Making prediction for datapoint 93 ...\n",
      "Making prediction for datapoint 94 ...\n",
      "Making prediction for datapoint 95 ...\n",
      "Making prediction for datapoint 96 ...\n",
      "Making prediction for datapoint 97 ...\n",
      "Making prediction for datapoint 98 ...\n",
      "Making prediction for datapoint 99 ...\n",
      "Making prediction for datapoint 100 ...\n",
      "Making prediction for datapoint 101 ...\n",
      "Making prediction for datapoint 102 ...\n",
      "Making prediction for datapoint 103 ...\n",
      "Making prediction for datapoint 104 ...\n",
      "Making prediction for datapoint 105 ...\n",
      "Making prediction for datapoint 106 ...\n",
      "Making prediction for datapoint 107 ...\n",
      "Making prediction for datapoint 108 ...\n",
      "Making prediction for datapoint 109 ...\n",
      "Making prediction for datapoint 110 ...\n",
      "Making prediction for datapoint 111 ...\n",
      "Making prediction for datapoint 112 ...\n",
      "Making prediction for datapoint 113 ...\n",
      "Making prediction for datapoint 114 ...\n",
      "Making prediction for datapoint 115 ...\n",
      "Making prediction for datapoint 116 ...\n",
      "Making prediction for datapoint 117 ...\n",
      "Making prediction for datapoint 118 ...\n",
      "Making prediction for datapoint 119 ...\n",
      "Making prediction for datapoint 120 ...\n",
      "Making prediction for datapoint 121 ...\n",
      "Making prediction for datapoint 122 ...\n",
      "Making prediction for datapoint 123 ...\n",
      "Making prediction for datapoint 124 ...\n",
      "Making prediction for datapoint 125 ...\n",
      "Making prediction for datapoint 126 ...\n",
      "Making prediction for datapoint 127 ...\n",
      "Making prediction for datapoint 128 ...\n",
      "Making prediction for datapoint 129 ...\n",
      "Making prediction for datapoint 130 ...\n",
      "Making prediction for datapoint 131 ...\n",
      "Making prediction for datapoint 132 ...\n",
      "Making prediction for datapoint 133 ...\n",
      "Making prediction for datapoint 134 ...\n",
      "Making prediction for datapoint 135 ...\n",
      "Making prediction for datapoint 136 ...\n",
      "Making prediction for datapoint 137 ...\n",
      "Making prediction for datapoint 138 ...\n",
      "Making prediction for datapoint 139 ...\n",
      "Making prediction for datapoint 140 ...\n",
      "Making prediction for datapoint 141 ...\n",
      "Making prediction for datapoint 142 ...\n",
      "Making prediction for datapoint 143 ...\n",
      "Making prediction for datapoint 144 ...\n",
      "Making prediction for datapoint 145 ...\n",
      "Making prediction for datapoint 146 ...\n",
      "Making prediction for datapoint 147 ...\n",
      "Making prediction for datapoint 148 ...\n",
      "Making prediction for datapoint 149 ...\n",
      "Making prediction for datapoint 150 ...\n",
      "Making prediction for datapoint 151 ...\n",
      "Making prediction for datapoint 152 ...\n",
      "Making prediction for datapoint 153 ...\n",
      "Making prediction for datapoint 154 ...\n",
      "Making prediction for datapoint 155 ...\n",
      "Making prediction for datapoint 156 ...\n",
      "Making prediction for datapoint 157 ...\n",
      "Making prediction for datapoint 158 ...\n",
      "Making prediction for datapoint 159 ...\n",
      "Making prediction for datapoint 160 ...\n",
      "Making prediction for datapoint 161 ...\n",
      "Making prediction for datapoint 162 ...\n",
      "Making prediction for datapoint 163 ...\n",
      "Making prediction for datapoint 164 ...\n",
      "Making prediction for datapoint 165 ...\n",
      "Making prediction for datapoint 166 ...\n",
      "Making prediction for datapoint 167 ...\n",
      "Making prediction for datapoint 168 ...\n",
      "Making prediction for datapoint 169 ...\n",
      "Making prediction for datapoint 170 ...\n",
      "Making prediction for datapoint 171 ...\n",
      "Making prediction for datapoint 172 ...\n",
      "Making prediction for datapoint 173 ...\n",
      "Making prediction for datapoint 174 ...\n",
      "Making prediction for datapoint 175 ...\n",
      "Making prediction for datapoint 176 ...\n",
      "Making prediction for datapoint 177 ...\n",
      "Making prediction for datapoint 178 ...\n",
      "Making prediction for datapoint 179 ...\n",
      "Making prediction for datapoint 180 ...\n",
      "Making prediction for datapoint 181 ...\n",
      "Making prediction for datapoint 182 ...\n",
      "Making prediction for datapoint 183 ...\n",
      "Making prediction for datapoint 184 ...\n",
      "Making prediction for datapoint 185 ...\n",
      "Making prediction for datapoint 186 ...\n",
      "Making prediction for datapoint 187 ...\n",
      "Making prediction for datapoint 188 ...\n",
      "Making prediction for datapoint 189 ...\n",
      "Making prediction for datapoint 190 ...\n",
      "Making prediction for datapoint 191 ...\n",
      "Making prediction for datapoint 192 ...\n",
      "Making prediction for datapoint 193 ...\n",
      "Making prediction for datapoint 194 ...\n",
      "Making prediction for datapoint 195 ...\n",
      "Making prediction for datapoint 196 ...\n",
      "Making prediction for datapoint 197 ...\n",
      "Making prediction for datapoint 198 ...\n",
      "Making prediction for datapoint 199 ...\n",
      "Making prediction for datapoint 200 ...\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "pred_labels = [] \n",
    "cnt = 1\n",
    "for i in range(len(titles)): \n",
    "    print(\"Making prediction for datapoint {} ...\".format(cnt))\n",
    "    title = titles[i] \n",
    "    splitted_content = split_text(clean_text(contents[i])) \n",
    "    pred_sum = 0 \n",
    "    for content in splitted_content: \n",
    "        input_id, attention_mask, token_type_id = electra_tokenizer_simple(title, content, MAX_LEN) \n",
    "        \n",
    "        input_id = torch.tensor(input_id) \n",
    "        attention_mask = torch.tensor(attention_mask) \n",
    "        token_type_id = torch.tensor(token_type_id)   \n",
    "        input_id = torch.reshape(input_id, (1,512)) \n",
    "        attention_mask = torch.reshape(attention_mask, (1,512)) \n",
    "        token_type_id = torch.reshape(token_type_id, (1,512))\n",
    "        with torch.no_grad(): \n",
    "            yhat = model(input_ids=input_id, attention_mask=attention_mask, \n",
    "                               token_type_ids = token_type_id)  \n",
    "        \n",
    "        p = torch.sigmoid(yhat[0][:,1]) \n",
    "        pred_sum += p \n",
    "    pred_avg = pred_sum / len(splitted_content) \n",
    "    pred_avg_val = pred_avg.item() \n",
    "    label = (1 if pred_avg_val > 0.5 else 0)     \n",
    "    pred_labels.append(label)    \n",
    "    cnt += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.asarray(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute accuracy \n",
    "accuracy = np.sum(pred_labels == y_test) / len(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 57.99999999999999%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using the head tail truncation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [] \n",
    "def compute_accuracy(model, dataloader, device):\n",
    "    tqdm()\n",
    "    model.eval()\n",
    "    correct_preds, num_samples = 0,0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            b_input_ids, b_input_masks, b_token_type_ids, b_labels = tuple(t.to(device) for t in batch) \n",
    "            loss, yhat = model(input_ids=b_input_ids, attention_mask=b_input_masks, \n",
    "                               token_type_ids = b_token_type_ids, labels=b_labels.long())\n",
    "            prediction = (torch.sigmoid(yhat[:,1]) > 0.5).long() \n",
    "            predictions.append(prediction)\n",
    "            num_samples += b_labels.size(0)\n",
    "            correct_preds += (prediction==b_labels.long()).sum()\n",
    "            del b_input_ids, b_input_masks, b_token_type_ids, b_labels #memory\n",
    "        torch.cuda.empty_cache() #memory\n",
    "        gc.collect() # memory \n",
    "        return correct_preds.float()/num_samples*100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./storage/minds_fake_news/mindslab_test.csv') \n",
    "test = test.loc[100:]\n",
    "y_test = test['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def electra_tokenizer_empirical(sent1, sent2, MAX_LEN):  \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent1, \n",
    "        text_pair = sent2,  \n",
    "        add_special_tokens = True, # add [CLS] and [SEP]\n",
    "        pad_to_max_length = False, \n",
    "        return_attention_mask = True # constructing attention_masks \n",
    "    )  \n",
    "    \n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] # differentiate padding from non padding \n",
    "    token_type_id = encoded_dict['token_type_ids'] # differentiate two sentences    \n",
    "    \n",
    "    if len(input_id) > 512: \n",
    "        input_id = input_id[:129] + input_id[-383:] \n",
    "        attention_mask = attention_mask[:129] + attention_mask[-383:]  \n",
    "        token_type_id = token_type_id[:129] + token_type_id[-383:]   \n",
    "    elif len(input_id) < 512: \n",
    "        input_id = input_id + [0]*(512 - len(input_id)) \n",
    "        attention_mask = attention_mask + [0]*(512 - len(attention_mask))\n",
    "        token_type_id = token_type_id + [0]*(512 - len(token_type_id))  \n",
    "        \n",
    "    return np.asarray(input_id), np.asarray(attention_mask), np.asarray(token_type_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Data preprocessing step - save the data so that we can just load it next time\n",
    "################################################################################\n",
    "MAX_LEN = 512\n",
    "input_ids = []\n",
    "attention_masks = [] \n",
    "token_type_ids = [] \n",
    "cnt = 0\n",
    "for sent1, sent2 in zip(test['title'], test['content']): \n",
    "    if cnt%1000 == 0 and cnt > 0: \n",
    "        print(\"Processed {} datapoints\".format(cnt)) \n",
    "    cnt += 1\n",
    "    try: \n",
    "        input_id, attention_mask, token_type_id = electra_tokenizer_empirical(sent1, sent2, MAX_LEN)\n",
    "        input_ids.append(input_id) \n",
    "        attention_masks.append(attention_mask) \n",
    "        token_type_ids.append(token_type_id) \n",
    "    except Exception as e:  \n",
    "        print(e)  \n",
    "        print(sent1, sent2) \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_ids) \n",
    "attention_masks = torch.tensor(attention_masks) \n",
    "token_type_ids = torch.tensor(token_type_ids)  \n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "test_data = TensorDataset(input_ids, attention_masks, token_type_ids, y_test) \n",
    "test_sampler = SequentialSampler(test_data) \n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size, shuffle = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")    \n",
    "checkpoint = torch.load('./storage/minds_fake_news/electra_M_8') \n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:00<00:01,  9.12it/s]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:00<00:01,  6.89it/s]\u001b[A\n",
      " 31%|███       | 4/13 [00:00<00:01,  6.10it/s]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:00<00:01,  5.68it/s]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:01<00:01,  5.48it/s]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:01<00:01,  5.36it/s]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:01<00:00,  5.28it/s]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:01<00:00,  5.23it/s]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:01<00:00,  5.20it/s]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:01<00:00,  5.18it/s]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:02<00:00,  5.16it/s]\u001b[A\n",
      "100%|██████████| 13/13 [00:02<00:00,  5.48it/s]\u001b[A\n",
      "0it [00:02, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 74.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set accuracy = {}\".format(compute_accuracy(model, test_dataloader, device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
